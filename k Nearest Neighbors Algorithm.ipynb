{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python code for k Nearest Neighbors Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implements a k-nearest neighbors classifier (kNN classifier) that can find the top-k nearest objects to a specific query object. Using this to classify text documents when a specific search text like ”The Bicycle Thief?” the algorithm can search in the document corpus and find the top K similar documents. \n",
    "\n",
    "Dataset Used is a Wikipedia data set. Each Wikipedia Page is a document and has a unique document ID and a specific URL. #### Sample of the dataset is available at \n",
    "##### https://s3.amazonaws.com/metcs777/WikipediaPagesOneDocPerLine1000LinesSmall.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generating the Top 20K dictionary words\n",
    "\n",
    "Below code gets the top 20,000 words in a local array and sorts them based on the frequency of words. The top 20K most frequent words in the corpus will act the dictionary \n",
    "\n",
    "At the end, we create  an RDD that includes the docID as key and a numpy array for the position of each words in the top 20K dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import re\n",
    "import numpy as np \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from pyspark import SparkContext\n",
    "import math\n",
    "from math import sqrt\n",
    "from math import isnan\n",
    "from operator import add\n",
    "\n",
    "\n",
    "def stringVector(x): \n",
    "    returnVal= str (x[0]) \n",
    "    for j in x[1]:\n",
    "        returnVal += ','+ str(j) \n",
    "    return returnVal\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: wordcount <file> \", file=sys.stderr)\n",
    "        exit(-1)\n",
    "        \n",
    "    sc = SparkContext(appName=\"Assig2_CS777\")\n",
    "    corpus = sc.textFile(sys.argv[1], 1)\n",
    "    numberOfDocs = corpus.count()\n",
    "    validLines = corpus.filter(lambda x : 'id=' in x and 'url=' in x)\n",
    "    numberOfValidLines = validLines.count()\n",
    "    keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "    keyAndListOfWords = keyAndText.map(lambda x: (str(x[0]),regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "    allWords= keyAndListOfWords.flatMap(lambda x: [x[0],[(data, 1) for data in x[1]]])\n",
    "\n",
    "\n",
    "    allWords= keyAndListOfWords.flatMap(lambda x: [(data, 1) for data in x[1]])\n",
    "\n",
    "    keyval= allWords.groupByKey()\n",
    "\n",
    "    wordsGrouped = allWords.groupByKey()\n",
    "\n",
    "    allWords.map(lambda x , y: (x , 1)).reduceByKey(lambda a,b: (a + b))\n",
    "\n",
    "    wordsGrouped = allWords.groupByKey()\n",
    "\n",
    "    allCounts = wordsGrouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "\n",
    "    topWords = allCounts.values().take(20000)\n",
    "\n",
    "    twentyK = sc.parallelize(range(20000))\n",
    "\n",
    "    dictionary = twentyK.map(lambda x: (topWords[x], x))\n",
    "\n",
    "    allWords = keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "\n",
    "    allDictionaryWords = dictionary.join(allWords)\n",
    "\n",
    "    justDocAndPos = allDictionaryWords.map( lambda x: ( x [1][1], x[1][0] ))\n",
    "\n",
    "    allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\n",
    "\n",
    "    new = allDictionaryWordsInEachDoc.map(lambda x : (x[0], list(x[1])))\n",
    "\n",
    "    forCSV = new.map(lambda x: ( x[0], np.array(x[1])))\n",
    "\n",
    "    forCSV = forCSV.map(lambda x: stringVector(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create the TF-IDF Array\n",
    "\n",
    "After having the top 20K words we create a large array that has columns as the word list in order and rows as documents.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def buildArray(listOfIndices):\n",
    "        returnVal = np.zeros(20000)\n",
    "        for index in listOfIndices:\n",
    "            returnVal[index] = returnVal[index] + 1\n",
    "        mysum = np.sum(returnVal)\n",
    "        returnVal = np.divide(returnVal, mysum)\n",
    "        return returnVal\n",
    "\n",
    "    allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: ( x[0], buildArray(x[1])))\n",
    "\n",
    "    def helper(arr):\n",
    "        newArray = np.zeros(20000)\n",
    "        for k in range(len(arr)):\n",
    "            if(arr[k] == 0):\n",
    "                newArray[k]= 0\n",
    "            else:\n",
    "                newArray[k]= 1\n",
    "        return newArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a version of allDocsAsNumpyArrays where all enteries in the array are either zero or one. A one indicates the presence of the word and a zero proves that the word does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    zeroOrOne = allDocsAsNumpyArrays.map(lambda x: (x[0], helper(x[1])))\n",
    "\n",
    "    dfArray = zeroOrOne.reduce(lambda x1, x2:(( \"\", np.add(x1[1], x2[1]))))\n",
    "    dfArray = dfArray[1]\n",
    "\n",
    "    multiplier = np.full(20000, numberOfDocs)\n",
    "    \n",
    "        idfArray = np.zeros(20000)\n",
    "\n",
    "    for k in range(len(dfArray)):\n",
    "        idfArray[k] = np.log(np.divide(numberOfDocs, float(dfArray[k])))\n",
    "\n",
    "    allDocsAsNumpyArraysTFidf = allDocsAsNumpyArrays.map (lambda x: (x[0], np.multiply (x[1], idfArray)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Implementing the getPrediction function\n",
    "\n",
    "Finally, we implement the function getPrediction (textInput, k). This function will predict the membership of the input text string in one of the 20 different newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we have a function that returns the prediction for the label of a string, using a kNN algorithm\n",
    "def getPrediction (textInput, k):\n",
    "    \n",
    "    # Create an RDD out of the textIput\n",
    "    myDoc = sc.parallelize(('', textInput))\n",
    "    \n",
    "    # Flat map the text to (word, 1) pair for each word in the doc\n",
    "    wordsInThatDoc = myDoc.flatMap (lambda x : ((j, 1) for j in regex.sub(' ', x).lower().split()))\n",
    "\n",
    "    # This will give us a set of (word, (dictionaryPos, 1)) pairs\n",
    "    allDictionaryWordsInThatDoc = dictionary.join (wordsInThatDoc).map (lambda x: (x[1][1], x[1][0])).groupByKey()\n",
    "\n",
    "    # Get tf array for the input string\n",
    "    myArray = buildArray(allDictionaryWordsInThatDoc.top(1)[0][1])\n",
    "\n",
    "    # Get the tf * idf array for the input string\n",
    "    myArray = np.multiply(myArray, idfArray)\n",
    "\n",
    "    # Get the distance from the input text string to all database documents, using cosine similarity (np.dot() )\n",
    "    distances = allDocsAsNumpyArraysTFidf.map(lambda x : (x[0], np.dot(x[1], myArray)))\n",
    "\n",
    "    # get the top k distances\n",
    "    topK = distances.top(k, lambda x : x[1])\n",
    "\n",
    "    # and transform the top k distances into a set of (docID, 1) pairs\n",
    "    docIDRepresented = sc.parallelize(topK).map (lambda x : (x[0], 1))\n",
    "\n",
    "    # now, for each docID, get the count of the number of times this document ID appeared in the top k\n",
    "    numTimes = docIDRepresented.reduceByKey(add)\n",
    "\n",
    "    # Return the top 1 of them.  \n",
    "    return numTimes.top(1, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Prediction of the Input Text \n",
    "\n",
    "Depolying the above code over Google Cloud Platform, we use the below Text to test the feasibility of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://en.wikipedia.org/wiki?curid=' + str(getPrediction('God and Religion', 1)[0][0]))\n",
    "print('https://en.wikipedia.org/wiki?curid=' + str(getPrediction('Sport Basketball Volleyball Soccer', 1)[0][0]))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing On a small dataset of 1000 documents, above query will fetch https://en.wikipedia.org/wiki?curid=433978 and  https://en.wikipedia.org/wiki?curid=418388 respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
